github仓库链接：https://github.com/Ustinaian/032002608
# 一、PSP表格
| PSP2.1 | Personal Software Process Stages | 预估耗时（分钟） | 实际耗时（分钟） |
| ---- | ---- |  ---- | ---- | 
|Planning|计划|30|120|
|· Estimate	|·估计这个任务需要多少时间	|4630	|7820|
|Development	|开发	|1440	|2860|
|· Analysis	|·需求分析（包括学习新技术）	|1800	|2310|
|· Design Spec	|·生成设计文档	|60	|180|
|· Design Review	|·设计复审	|30	|60|
|· Coding Standard	|·代码规范（为目前的开发制定合适的规范）	|40	|120|
|· Design	|·具体设计|120|	250|
|· Coding	|·具体编码|	600|	1200|
|· Code Review|	·代码复审|	60	|190|
|· Test	|·测试（自我测试，修改代码，提交修改）|	120|	160|
|Reporting	|报告	|130	|180|
|· Test Report	|·测试报告	|120	|170|
|· Size Measurement	|·计算工作量	|30	|50|
|·Postmortem & Process Improvement Plan	|·事后总结，并提出过程改进计划	|80	|90|
|	|·合计	|4630	|7820|
# 二、任务要求的实现
## 1. 项目设计与技术栈
**阅读完题目后，我将任务分成了8个环节，分别是：**
* 理解题目，深入挖掘题目的要求；
* 分析任务，查找需要学习的资料与技术，以便后续学习；
* 爬取卫健委官网中国大陆每日本土新增确诊人数及新增无症状感染人数；
* 爬取所有省份包括港澳台每日本土新增确诊人数及新增无症状感染人数；
* 提取数据并写入excel表格；
* 利用**LSTM**,即Long Short Term Memory Network，进行每日热点分析；
* 实现数据可视化；
* 最后撰写博客
  
本次任务我使用**python**实现。在爬取数据阶段，借鉴了网络上的博客，学习了**pyppeteer**库以及**BeautifulSoup**，用它们完成了爬虫任务；在提取数据阶段使用了正则表达式进行提取数据，并使用xlwt库实现对excel表格的编辑；在热点分析阶段，使用LSTM进行值得关注事件的挖掘；最后使用pyecharts库完成了数据可视化。

本次任务使用的技术栈有：**pyppeteer、BeautifulSoup、正则表达式、xlwt、xlrd、os、pyecharts、git、github、re、asyncio**

## 2. 爬虫与数据处理
- **爬虫阶段共7个函数**
* 首先使用fetchurl函数发起网络请求，获取网页源码，其中绕过卫健委网站的反爬机制是关键，可以将进程伪装成浏览器
  ![](https://img2022.cnblogs.com/blog/2973217/202209/2973217-20220920175745410-1267548678.png)

* 接着使用getpageurl函数从1到42页获取每一页的url链接
  ![](https://img2022.cnblogs.com/blog/2973217/202209/2973217-20220920175931551-1618725090.png)

* 然后使用getTitleUrl函数获取某一页的文章列表中的每一篇文章的标题，链接，和发布日期
  ![](https://img2022.cnblogs.com/blog/2973217/202209/2973217-20220920180101946-756090051.png)

* 再然后使用getContent函数获取某一篇文章的正文内容
  ![](https://img2022.cnblogs.com/blog/2973217/202209/2973217-20220920180330294-496242209.png)

* 最后使用savefile函数，以爬取到的日期为文件名，将爬取到的数据保存到txt文档里
  ![](https://img2022.cnblogs.com/blog/2973217/202209/2973217-20220920180524516-66917489.png)

* main函数
  ![](https://img2022.cnblogs.com/blog/2973217/202209/2973217-20220920180615865-1179195882.png)

* **数据处理阶段共8个函数，分为5个模块，分别是：**
    **1. 提取本土新增确诊和新增无症状数据模块**
  * 使用readfile0读取文件并提取本土新增确诊和新增无症状数据，使用save_data0函数写入数据再保存excel文件
  ![](https://img2022.cnblogs.com/blog/2973217/202209/2973217-20220920182650806-590382423.png)
  * 使用save_data0将新增确诊和新增无症状数据写入表单
  ![](https://img2022.cnblogs.com/blog/2973217/202209/2973217-20220920182946156-1576843763.png)
  
  **2. 提取各省新增确诊数据模块**
  * 使用readfile1读取文件并提取各省新增确诊数据，使用save_data1函数写入数据再保存excel文件
  ![](https://img2022.cnblogs.com/blog/2973217/202209/2973217-20220920183503481-1006851429.png)
  * 使用save_data1将各省新增确诊数据写入表单
  ![](https://img2022.cnblogs.com/blog/2973217/202209/2973217-20220920183657263-1267372033.png)

  **3. 提取各省新增无症状数据模块**
  * 使用readfile2读取文件并提取各省新增无症状数据，使用save_data2函数写入数据再保存excel文件
  ![](https://img2022.cnblogs.com/blog/2973217/202209/2973217-20220920183914053-2030513637.png)
  * 使用save_data2将各省新增无症状数据写入表单
  ![](https://img2022.cnblogs.com/blog/2973217/202209/2973217-20220920184007837-723911395.png)

  **4. 提取港澳台累计确诊数据模块**
  * 使用readfile3读取文件并提取港澳台累计确诊数据，然后写入表单，保存excel文件
  ![](https://img2022.cnblogs.com/blog/2973217/202209/2973217-20220920184216271-1945092931.png)

  **5. 主函数模块**
  * 调用函数，实现数据提取以及存储
  ![](https://img2022.cnblogs.com/blog/2973217/202209/2973217-20220920184341429-986542799.png)

## 3. 数据统计接口部分的性能改进
![](https://img2022.cnblogs.com/blog/2973217/202209/2973217-20220920204328899-236184238.png)
![](https://img2022.cnblogs.com/blog/2973217/202209/2973217-20220920204406829-2090260166.png)
![](https://img2022.cnblogs.com/blog/2973217/202209/2973217-20220920204430171-124603200.png)
![](https://img2022.cnblogs.com/blog/2973217/202209/2973217-20220920204450975-1812743835.png)
![](https://img2022.cnblogs.com/blog/2973217/202209/2973217-20220920204506919-998797700.png)

![](https://img2022.cnblogs.com/blog/2973217/202209/2973217-20220920204111156-1833546941.png)

* 性能改进思路：使用多线程、线程池+异步调用
* 多线程会占据系统资源，太多的线程会减低系统对外界响应效率
* 采用线程池可以减少创建和销毁线程的频率，维持若干线程，并让空闲的线程重新承担新的执行任务

**消耗最大的函数**
![](https://img2022.cnblogs.com/blog/2973217/202209/2973217-20220920203537505-1739091807.png)


## 4. 每日热点的实现思路
* 使用**LSTM**实现，即长短时记忆网络，Long Short Term Memory Network，是一种改进之后的循环神经网络，可以解决RNN无法处理长距离的依赖的问题。原始**RNN**的隐藏层只有一个状态，即h，它对于短期的输入非常敏感。再增加一个状态，即c，让它来**保存长期的状态**，称为**单元状态**。在 t 时刻，LSTM 的输入有三个：当前时刻网络的输入值 X_t、上一时刻 LSTM 的输出值 h_t-1、以及上一时刻的单元状态 C_t-1；LSTM 的输出有两个：当前时刻 LSTM 输出值 h_t、和当前时刻的单元状态 C_t。

## 5. 数据可视化界面的展示
* 数据可视化设计**本土新增数据可视化、各省新增确诊数据可视化、各省新增无症状数据可视化**和**港澳台累计确诊数据可视化**，使用了柱状图进行展示，利用python的pyecharts库，使用了其中的Bar模块。
* 首先打开数据提取的excel，遍历每一行，将其作为一个列表，并将列表第一个元素作为地区压入存放地区数组**areas**，第二个元素作为新增确诊（或新增无症状、港澳台累计）数据压入数组**datas**。其中实现本土新增数据可视化时，还需将第三个元素作为新增无症状数据压入数组**datas2**。
* 然后以地区为x轴，各种新增数据为y轴，创建出4种可视化柱状图，并保存为html文件。
* **本土新增可视化代码**
  ![](https://img2022.cnblogs.com/blog/2973217/202209/2973217-20220920195304686-397836013.png)
* **本土新增可视化示例**
  ![](https://img2022.cnblogs.com/blog/2973217/202209/2973217-20220920195552793-1898241083.png)
* **各省新增确诊数据可视化代码**
  ![](https://img2022.cnblogs.com/blog/2973217/202209/2973217-20220920195652037-70882688.png)
* **各省新增确诊数据可视化示例**
  ![](https://img2022.cnblogs.com/blog/2973217/202209/2973217-20220920195738163-166315525.png)
* **各省新增无症状数据可视化代码**
  ![](https://img2022.cnblogs.com/blog/2973217/202209/2973217-20220920195905251-487055494.png)
* **各省新增无症状数据可视化示例**
  ![](https://img2022.cnblogs.com/blog/2973217/202209/2973217-20220920195946170-1485687758.png)
* **港澳台累计确诊数据可视化代码**
  ![](https://img2022.cnblogs.com/blog/2973217/202209/2973217-20220920200021786-1246409744.png)
* **港澳台累计确诊数据可视化示例**
  ![](https://img2022.cnblogs.com/blog/2973217/202209/2973217-20220920200106347-357569615.png)

# 三、心得体会
* 对我来说本次作业的难度还是有些大，曾经并没有系统地学习过python语言，只有零零散散地学过一些语法和**基础爬虫**，然而这些对于这次作业的帮助微乎其微，只好从头学起。
* 先是把**python语法**、**python数据结构**以及一些**常用库**都了解了一下，但是文档里全都是陌生的技术，还需要重新学习，再实际应用。
* 特别是爬虫，一开始我尝试用过去学习的**requests**进行爬取数据，发现拿不到我想要的内容，甚至与网上所教的**json**分析相差甚远，完全找不到方向。
* 后来跟同学交流后，发现卫健委的官网**反爬机制**很高，普通的爬虫程序无法攻破它，于是在同学的介绍下才开始使用python的**pyppeteer**库，上网查询资料，看博客后，才慢慢地有点苗头。
* 对于**机器学习**更是从未接触过，根本就感觉无从下手，今后在**机器学习**、**深度学习**等方面还需加强学习。
> 从这次作业中我深刻认识到自己对于一些技术的掌握还是不到位，还有非常多的技术需要学习和应用，也认识自己与同学们的差距，今后会加强自己在**程序开发**，**python实践**方面的学习，正好开发程序也是我的兴趣所在，应该会学习到更多的知识。另外，这次作业中，我对**时间**的安排还是不够**细致**，**效率**也不够高，一开始天真地认为第一次作业应该难度不大，总是在拖延，直到第一周末打开文档才发现，难度不是我所能想像的，这才开始紧张起来，但后期**学习效率**还不够高，在后面的作业中应当更完整、细致的安排每一阶段所需完成的任务，高效地**学习新技术**，并完成**开发**。